/**
 * Vulnerability Dataset Manager
 *
 * Handles the creation, updating, and management of security vulnerability datasets
 * for machine learning model training and evaluation.
 */

const fs = require('fs-extra');
const path = require('path');
const crypto = require('crypto');

// Default configuration
let config = {
  datasetPath: null,
  labeledDataPath: null,
  schemaVersion: '1.0.0',
  minItemsForTraining: 100,
  testSplitRatio: 0.2,
  validationSplitRatio: 0.1,
};

/**
 * Initialize the vulnerability dataset manager
 * @param {Object} userConfig - Configuration options
 * @returns {Promise<boolean>} Success status
 */
async function initialize(userConfig = {}) {
  try {
    // Update configuration with user settings
    config = {
      ...config,
      ...userConfig,
      datasetPath:
        userConfig.datasetPath || path.join(userConfig.dataStoragePath || '', 'datasets'),
      labeledDataPath:
        userConfig.labeledDataPath || path.join(userConfig.dataStoragePath || '', 'labeled'),
    };

    // Ensure required directories exist
    await fs.ensureDir(config.datasetPath);
    await fs.ensureDir(config.labeledDataPath);
    await fs.ensureDir(path.join(config.datasetPath, 'training'));
    await fs.ensureDir(path.join(config.datasetPath, 'testing'));
    await fs.ensureDir(path.join(config.datasetPath, 'validation'));

    return true;
  } catch (error) {
    console.error('Failed to initialize vulnerability dataset manager:', error);
    return false;
  }
}

/**
 * Create a new vulnerability dataset from labeled data
 * @param {Object} options - Dataset creation options
 * @returns {Promise<Object>} Dataset creation results
 */
async function createDataset(options = {}) {
  const {
    name = `vulnerability-dataset-${Date.now()}`,
    description = 'Web3 vulnerability dataset',
    sourceFiles = [],
    sourceDirectories = [],
    includeAllLabeled = true,
    categories = [],
    severities = [],
    outputPath = null,
    metadataFields = ['category', 'severity', 'platform', 'context'],
  } = options;

  try {
    // Generate dataset identifier
    const datasetId = `dataset-${crypto.randomBytes(4).toString('hex')}`;
    const timestamp = new Date().toISOString();

    // Set dataset output path
    const datasetOutputPath = outputPath || path.join(config.datasetPath, `${name}-${datasetId}`);
    await fs.ensureDir(datasetOutputPath);

    // Collect vulnerability items
    const items = [];

    // Process all labeled data if requested
    if (includeAllLabeled) {
      const labeledFiles = await fs.readdir(config.labeledDataPath);
      for (const file of labeledFiles.filter(f => f.endsWith('.json'))) {
        const filePath = path.join(config.labeledDataPath, file);
        const labeledData = await fs.readJson(filePath);
        items.push(...filterVulnerabilityItems(labeledData, { categories, severities }));
      }
    }

    // Process specific source files
    for (const filePath of sourceFiles) {
      if (await fs.pathExists(filePath)) {
        const data = await fs.readJson(filePath);
        items.push(...filterVulnerabilityItems(data, { categories, severities }));
      }
    }

    // Process source directories
    for (const dirPath of sourceDirectories) {
      if (await fs.pathExists(dirPath)) {
        const files = await fs.readdir(dirPath);
        for (const file of files.filter(f => f.endsWith('.json'))) {
          const filePath = path.join(dirPath, file);
          const data = await fs.readJson(filePath);
          items.push(...filterVulnerabilityItems(data, { categories, severities }));
        }
      }
    }

    // Create dataset metadata
    const metadata = {
      id: datasetId,
      name,
      description,
      created: timestamp,
      itemCount: items.length,
      version: config.schemaVersion,
      categories: countByProperty(items, 'category'),
      severities: countByProperty(items, 'severity'),
      stats: {
        sourceCount: sourceFiles.length + sourceDirectories.length + (includeAllLabeled ? 1 : 0),
        itemsPerCategory: countByProperty(items, 'category'),
        itemsPerSeverity: countByProperty(items, 'severity'),
        itemsPerPlatform: countByProperty(items, 'platform'),
      },
    };

    // Split items into training, testing, validation sets
    const splitSets = splitDataset(items);

    // Prepare dataset files
    const datasetFiles = [
      { name: 'metadata.json', data: metadata },
      { name: 'training-set.json', data: { items: splitSets.training }, path: 'training' },
      { name: 'testing-set.json', data: { items: splitSets.testing }, path: 'testing' },
      { name: 'validation-set.json', data: { items: splitSets.validation }, path: 'validation' },
    ];

    // Write dataset files
    for (const file of datasetFiles) {
      const filePath = file.path
        ? path.join(datasetOutputPath, file.path, file.name)
        : path.join(datasetOutputPath, file.name);

      await fs.ensureDir(path.dirname(filePath));
      await fs.writeJson(filePath, file.data, { spaces: 2 });
    }

    // Return dataset information
    return {
      id: datasetId,
      name,
      description,
      path: datasetOutputPath,
      itemCount: items.length,
      sets: {
        training: splitSets.training.length,
        testing: splitSets.testing.length,
        validation: splitSets.validation.length,
      },
      metadata,
    };
  } catch (error) {
    console.error('Error creating vulnerability dataset:', error);
    return {
      error: error.message,
      status: 'failed',
    };
  }
}

/**
 * Filter vulnerability items based on specified criteria
 * @param {Array} items - Vulnerability items to filter
 * @param {Object} criteria - Filter criteria
 * @returns {Array} Filtered items
 */
function filterVulnerabilityItems(items, criteria = {}) {
  const { categories = [], severities = [] } = criteria;

  if (!Array.isArray(items)) {
    return [];
  }

  return items.filter(item => {
    // Apply category filter if specified
    if (categories.length > 0 && !categories.includes(item.category)) {
      return false;
    }

    // Apply severity filter if specified
    if (severities.length > 0 && !severities.includes(item.severity)) {
      return false;
    }

    return true;
  });
}

/**
 * Split dataset into training, testing, and validation sets
 * @param {Array} items - Dataset items
 * @returns {Object} Split datasets
 */
function splitDataset(items) {
  // Shuffle items for randomized splitting
  const shuffled = [...items].sort(() => 0.5 - Math.random());

  // Calculate set sizes
  const testSize = Math.round(shuffled.length * config.testSplitRatio);
  const validationSize = Math.round(shuffled.length * config.validationSplitRatio);
  const trainingSize = shuffled.length - testSize - validationSize;

  // Split items into sets
  return {
    training: shuffled.slice(0, trainingSize),
    testing: shuffled.slice(trainingSize, trainingSize + testSize),
    validation: shuffled.slice(trainingSize + testSize),
  };
}

/**
 * Count items by a specific property value
 * @param {Array} items - Items to count
 * @param {string} property - Property to count by
 * @returns {Object} Counts by property value
 */
function countByProperty(items, property) {
  const counts = {};

  items.forEach(item => {
    const value = item[property] || 'unknown';
    counts[value] = (counts[value] || 0) + 1;
  });

  return counts;
}

/**
 * List available vulnerability datasets
 * @returns {Promise<Array>} List of available datasets
 */
async function listDatasets() {
  try {
    const results = [];
    const entries = await fs.readdir(config.datasetPath, { withFileTypes: true });

    for (const entry of entries) {
      if (entry.isDirectory()) {
        const metadataPath = path.join(config.datasetPath, entry.name, 'metadata.json');

        if (await fs.pathExists(metadataPath)) {
          const metadata = await fs.readJson(metadataPath);
          results.push({
            id: metadata.id,
            name: metadata.name,
            description: metadata.description,
            itemCount: metadata.itemCount,
            created: metadata.created,
            path: path.join(config.datasetPath, entry.name),
          });
        }
      }
    }

    return results;
  } catch (error) {
    console.error('Error listing vulnerability datasets:', error);
    return [];
  }
}

/**
 * Get detailed information about a specific dataset
 * @param {string} datasetId - Dataset identifier
 * @returns {Promise<Object>} Dataset information
 */
async function getDatasetInfo(datasetId) {
  try {
    const datasets = await listDatasets();
    const dataset = datasets.find(d => d.id === datasetId);

    if (!dataset) {
      throw new Error(`Dataset with ID ${datasetId} not found`);
    }

    const metadataPath = path.join(dataset.path, 'metadata.json');
    const metadata = await fs.readJson(metadataPath);

    // Get training/testing/validation set counts
    const trainingPath = path.join(dataset.path, 'training', 'training-set.json');
    const testingPath = path.join(dataset.path, 'testing', 'testing-set.json');
    const validationPath = path.join(dataset.path, 'validation', 'validation-set.json');

    const trainingSet = (await fs.pathExists(trainingPath))
      ? await fs.readJson(trainingPath)
      : { items: [] };
    const testingSet = (await fs.pathExists(testingPath))
      ? await fs.readJson(testingPath)
      : { items: [] };
    const validationSet = (await fs.pathExists(validationPath))
      ? await fs.readJson(validationPath)
      : { items: [] };

    return {
      ...metadata,
      setStats: {
        training: trainingSet.items.length,
        testing: testingSet.items.length,
        validation: validationSet.items.length,
      },
      path: dataset.path,
    };
  } catch (error) {
    console.error(`Error getting dataset info for ${datasetId}:`, error);
    return {
      error: error.message,
      status: 'failed',
    };
  }
}

module.exports = {
  initialize,
  createDataset,
  listDatasets,
  getDatasetInfo,
};
